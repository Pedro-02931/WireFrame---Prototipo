
# ğŸ–¥ï¸ MÃ³dulo Servidor â€“ Sistema de GestÃ£o LogÃ­stica

## ğŸš€ VisÃ£o Geral
O **MÃ³dulo Servidor** Ã© o **nÃºcleo da operaÃ§Ã£o logÃ­stica**, responsÃ¡vel por **processar, armazenar e fornecer dados** para o **Coletor** e **Dashboard**.  

Ele gerencia as solicitaÃ§Ãµes, valida informaÃ§Ãµes dos pacotes, controla permissÃµes e **mantÃ©m a IA treinada com feedbacks dos usuÃ¡rios**.

---

## ğŸ“œ Fluxo de OperaÃ§Ã£o

1ï¸âƒ£ **Recebimento de RequisiÃ§Ãµes**
   - O servidor escuta **solicitaÃ§Ãµes do Coletor e Dashboard**.
   - Verifica permissÃµes e autenticaÃ§Ã£o antes de processar os dados.

2ï¸âƒ£ **ValidaÃ§Ã£o de Dados**
   - Quando um cÃ³digo de barras Ã© lido pelo Coletor, o servidor verifica se **os dados do pacote jÃ¡ existem no banco**.
   - Caso contrÃ¡rio, **gera um novo registro e sugere um setor de destino**.
   - Caso seja o ultimo, **define um TTL** para correÃ§Ãµes.

3ï¸âƒ£ **InferÃªncia e Aprendizado da IA**
   - O sistema sugere um destino para o pacote com base em **dados histÃ³ricos e regras logÃ­sticas**.
   - Caso o usuÃ¡rio corrija a inferÃªncia, o servidor **ajusta automaticamente o modelo de IA**.

4ï¸âƒ£ **AtualizaÃ§Ã£o do Banco de Dados**
   - ApÃ³s a confirmaÃ§Ã£o ou correÃ§Ã£o, os dados sÃ£o **salvos no banco NoSQL**.
   - O **ciclo de vida do pacote** Ã© gerenciado para evitar sobrecarga.

5ï¸âƒ£ **Fornecimento de Dados ao Dashboard**
   - O servidor atende Ã s solicitaÃ§Ãµes do Dashboard para consultas, filtros e impressÃ£o de NFs.
   - A LLM integrada no chat do Dashboard **consulta dados relevantes** e responde perguntas sobre pedidos e estoque.

---

## âš™ï¸ MÃ©todos e Funcionalidades
### Geral
```mermaid
classDiagram
    class Servidor {
        +ouvirRequisicoes()
        +validarDadosPacote(codigo: string)
        +sugerirDestino(dadosPacote: object)
        +processarFeedback(erro: boolean, dadosCorrigidos: object)
        +atualizarModeloIA(erro: boolean)
        +armazenarDados(dadosPacote: object)
        +consultarPedidos(query: object)
        +gerenciarSeguranca(autenticacao: object)
        +interfaces(credencial: object)
        +gerarEmbedding(heuristicas: string)
        +transmitirEmbedding()
    }

    class Coletor {
        +autenticar(credencial: object)
        +lerCodigoBarras(codigo: string)
        +baixarDadosPacote(codigo: string)
        +enviarFeedbackServidor(correto: boolean, dadosCorretos: object)
    }

    class Dashboard {
        +autenticar(credencial: object)
        +buscarPedido(consulta: string)
        +filtrarPedidos(filtro: object)
        +baixarEmbeddings(APIservidor)
        +consultarChatIA(heuristicas: string, embedding: object)
        +gerarRelatorioAutomatico()
    }

    Servidor --> Coletor : Retorna Dados do Pacote
    Servidor --> Dashboard : Disponibiliza InformaÃ§Ãµes
    Coletor --> Servidor : Envia Feedback de CorreÃ§Ã£o
    Dashboard --> Servidor : Consulta e Filtra Pedidos
    Dashboard --> Servidor : Baixa Embeddings do Servidor
    Servidor --> Dashboard : Transmite Embeddings
    Dashboard --> ChatIA : Usa Embeddings Localmente

```
#### Por que rodar o LLM localmente?
âœ… Rodar a LLM localmente no notebook reduz carga no servidor.
âœ… TranspilaÃ§Ã£o vetorial Ã© mais leve, pois apenas gera representaÃ§Ãµes compactadas dos dados e nÃ£o requer inferÃªncia pesada no servidor.
âœ… Notebook pode operar offline, usando os embeddings baixados para consultas rÃ¡pidas.
**Traduzindo**
âœ… Menos carga no servidor â†’ Apenas processa embeddings e os transmite.
âœ… Dashboard pode funcionar offline â†’ LLM usa os embeddings locais.
âœ… Baixa latÃªncia â†’ UsuÃ¡rio acessa informaÃ§Ãµes rapidamente sem depender de rede.
âœ… Treinamento dinÃ¢mico â†’ O servidor ajusta embeddings ao longo do tempo, permitindo aprendizado incremental.
### Metodo de Treinamento
```mermaid
graph TD;
    
    %% Entrada dos Dados
    Pacote[Pacote Recebido] -->|Lido pelo Coletor| Inferencia[Modulo de InferÃªncia]
    
    %% InferÃªncia AutomÃ¡tica
    Inferencia -->|Baseado em histÃ³rico e caracterÃ­sticas| DestinoSugerido[Destino Sugerido]
    Inferencia -.->|Registro de decisÃ£o| BancoErros[(Banco de Dados de Erros)]

    %% ConfirmaÃ§Ã£o ou CorreÃ§Ã£o
    DestinoSugerido -->|ConfirmaÃ§Ã£o Humana| Registro[Registro no Servidor]
    DestinoSugerido -->|CorreÃ§Ã£o Manual| Correcao[Erro detectado]
    Correcao -->|Penaliza inferÃªncia e reenvia aprendizado| BancoErros

    %% AtualizaÃ§Ã£o da InferÃªncia
    BancoErros -->|Treino ContÃ­nuo| Inferencia

    %% ConversÃ£o para Embeddings e TransferÃªncia para Dashboard
    Registro -->|TransformaÃ§Ã£o em Texto| ProcessadorEmbeddings[ConversÃ£o para Embeddings]
    ProcessadorEmbeddings -->|GeraÃ§Ã£o do Arquivo Vetorial| EmbeddingArquivo[Arquivo de Embeddings]
    EmbeddingArquivo -->|Baixado pelo Dashboard| DashboardNotebook[Dashboard no Notebook]

    %% Processamento Local no Notebook
    DashboardNotebook -->|Carrega Embeddings Localmente| MotorLLM[LLM Local no Notebook]
    MotorLLM -->|Disponibiliza IA para UsuÃ¡rio| ChatIA[Chat LLM Integrado no Dashboard]
    MotorLLM -->|Gera RelatÃ³rios AutomÃ¡ticos| Relatorio[RelatÃ³rio Inteligente]

```
#### Por que nÃ£o rodar a IA no lado servidor?
âœ… Maior desempenho: O servidor apenas processa embeddings, sem rodar inferÃªncia direta na LLM.
âœ… ReduÃ§Ã£o de LatÃªncia: Dashboard carrega os embeddings uma Ãºnica vez, sem necessidade de acessar o servidor continuamente.
âœ… Modo Offline: A LLM roda mesmo sem conexÃ£o com o servidor.
âœ… RelatÃ³rios Inteligentes: A IA pode resumir e gerar relatÃ³rios baseados nos dados recebidos.
---

## ğŸ¯ BenefÃ­cios da AutomaÃ§Ã£o no Servidor
âœ… **InferÃªncia inteligente** â€“ O servidor **aprende com os erros** e ajusta o destino automaticamente.  
âœ… **ReduÃ§Ã£o de carga no banco** â€“ ImplementaÃ§Ã£o de **ciclo de vida dos pacotes** para escalabilidade.  
âœ… **InteraÃ§Ã£o com IA** â€“ O chat do Dashboard **consulta o servidor** para fornecer respostas baseadas nos dados atravÃ©s de embeddings e vetores.  
âœ… **SeguranÃ§a e Controle** â€“ O sistema **valida permissÃµes** antes de processar requisiÃ§Ãµes.  
